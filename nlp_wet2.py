# -*- coding: utf-8 -*-
"""NLP_Wet2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1b09TbwVyMQeD0v3luXddIscnZc-43st4
"""
import torch
from torch import nn, cuda, tensor, zeros, cat
from torch.optim import Adam, Adagrad
from torch.utils.data import DataLoader, Dataset
import torch.nn.functional as F
import numpy as np
import torchtext
from collections import defaultdict, OrderedDict
from itertools import combinations
from chu_liu_edmonds import decode_mst
from datetime import datetime
# from send_email import send_email

train_path = "train.labeled"
test_path = "test.labeled"
ROOT = "_R_"

def get_vocab(sentences):
  word_dict = defaultdict(int)
  pos_dict = defaultdict(int)
  for sentence in sentences:
    for i in range(len(sentence)):
      word, pos = sentence[i][0], sentence[i][1]
      word_dict[word] += 1
      pos_dict[pos] += 1
  return word_dict, pos_dict

class ParsingDataset(Dataset):
    def __init__(self, sentences):
        self.sentences = sentences

        self.word_count, self.pos_count = get_vocab(train_nohead + test_nohead)

        # Set word id from vocab
        self.word_idx = dict()
        self.idx_word = dict()
        self.words_list = list(self.word_count.keys())
        for i in range(len(self.words_list)):
            w = self.words_list[i]
            self.word_idx[w] = i
            self.idx_word[i] = w

        # Set POS id from vocab
        self.pos_idx = dict()
        self.idx_pos = dict()
        self.pos_list = list(self.pos_count.keys())
        for i in range(len(self.pos_list)):
            pos = self.pos_list[i]
            self.pos_idx[pos] = i
            self.idx_pos[i] = pos

        self.word_vocab_size, self.pos_vocab_size = len(self.words_list), len(self.pos_list)

    def vectorize_tokens(self, tokens):
        idxs = [self.word_idx[w] for w in tokens]
        return tensor([idxs])

    def vectorize_pos(self, pos):
        pos_vector = torch.tensor([self.pos_idx[p] for p in pos])
        # pos_vector = zeros((len(pos), len(self.pos_idx.keys())), dtype=torch.int32)
        # for i in range(len(pos)):
            # pos_vector[(i, self.pos_idx[pos[i]])] = 1
        return pos_vector

    def __len__(self):
        return len(self.sentences)

    def __getitem__(self, idx):
        s = self.sentences[idx]
        tokens_vector = self.vectorize_tokens([w[0] for w in s])
        pos_vector = self.vectorize_pos([w[1] for w in s])
        # arcs = [(int(s[i][2]), i) for i in range(len(s))]
        arcs = [int(s[i][2]) for i in range(len(s))]
        return (tokens_vector, pos_vector), tensor(arcs)

def extract_sentences(file_path):
    sentences = []
    with open(file_path, 'r') as f:
        cur_sentence = [(ROOT, ROOT, -1)]
        for line in f:
            if line != '\n':
                splitted = line.split('\t')
                cur_sentence.append((splitted[1], splitted[3], splitted[6]))
            else:
                sentences.append(cur_sentence)
                cur_sentence = [(ROOT, ROOT, -1)]
    return sentences

train_sentences = extract_sentences(train_path)
test_sentences = extract_sentences(test_path)

train_nohead = []
for s in train_sentences:
    _s = [(x[0], x[1]) for x in s]
    train_nohead.append(_s)

test_nohead = []
for s in test_sentences:
    _s = [(x[0], x[1]) for x in s]
    test_nohead.append(_s)

class DependencyParsingNetwork(nn.Module):
    def __init__(self, hidden_dim, word_vocab_size, word_embedding_dim, pos_embedding_dim, pos_vocab_size):
        super(DependencyParsingNetwork, self).__init__()
        self.word_embedding_dim = word_embedding_dim
        self.word_embedding = nn.Embedding(word_vocab_size, word_embedding_dim, padding_idx=0)
        self.pos_embedding = nn.Embedding(pos_vocab_size, pos_embedding_dim)
        # TODO: add dropout arg to lstm
        self.lstm = nn.LSTM(input_size=word_embedding_dim + pos_embedding_dim, hidden_size=hidden_dim, num_layers=2, bidirectional=True, batch_first=True)
        self.mlp = nn.Sequential(
            nn.Linear(2 * 2 * HIDDEN_DIM, 1),
            nn.Tanh(),
        )
        self.log_softmax = nn.LogSoftmax(dim=0)

    def forward(self, token_vector, pos_vector):
        _tokens = self.word_embedding(token_vector).squeeze(0)
        _pos = self.pos_embedding(pos_vector).squeeze(0)
        x = cat((_tokens, _pos), dim=1)
        # x = cat((self.word_embedding(token_vector).squeeze(0), self.pos_embedding(pos_vector).squeeze(0)) , dim=1)
        x = x.unsqueeze(0)
        x, (hn, cn) = self.lstm(x)
        x = x.squeeze(0)
        scores = zeros(x.shape[0], x.shape[0])
        for _i, _j in combinations(range(len(x)), 2):
            scores[_i][_j] = self.mlp(cat((x[_i],x[_j])))
        _scores = self.log_softmax(scores)
        return scores, _scores

# def vectorize_tokens(tokens, to_idx):
#     idxs = [to_idx[w] for w in tokens]
#     return tensor([idxs])


# def vectorize_pos(pos, to_idx):
#     pos_vector = zeros((len(pos), len(to_idx.keys())), dtype=int)
#     for i in range(len(pos)):
#         pos_vector[(i, to_idx[pos[i]])] = 1
#     return pos_vector

TRIM_TRAIN_DATASET = 50

if TRIM_TRAIN_DATASET > 0:
    train_dataset = ParsingDataset(train_sentences[:TRIM_TRAIN_DATASET])
else:
    train_dataset = ParsingDataset(train_sentences)

device = 'cuda' if cuda.is_available() else 'cpu'
print("Device = ", device)

HIDDEN_DIM = 125
WORD_EMBEDDING_DIM = 200
POS_EMBEDDING_DIM = 20
EPOCHS = 200
GRAD_STEPS = 3
BATCH_SIZE = 10
LEARNING_RATE = 1e-3


if __name__ == "__main__":
    model_file_name = "trained.model"
    model = DependencyParsingNetwork(HIDDEN_DIM, train_dataset.word_vocab_size,\
            WORD_EMBEDDING_DIM, POS_EMBEDDING_DIM,  train_dataset.pos_vocab_size)
    try:
        model.load_state_dict(torch.load(model_file_name))
    except FileNotFoundError:
        print("No saved model state, starting a new model")
    
    model = model.to(device)

    optimizer = Adagrad(model.parameters(), lr=LEARNING_RATE)
    # loss_function = nn.NLLLoss()
    # log_softmax = nn.LogSoftmax(dim=0)

    for epoch in range(EPOCHS):
        L = 0
        edges_count = 0
        correct_edges_count = 0
        random_batch_idx = torch.randint(len(train_dataset), (BATCH_SIZE,))
        _i = -1
        for i in random_batch_idx:
            _i += 1
            (tokens_vector, pos_vector), arcs = train_dataset[i]
            tokens_vector = tokens_vector.to(device)
            pos_vector = pos_vector.to(device)
            arc = arcs.to(device)

            # Forward
            scores, log_softmax_scores = model(tokens_vector, pos_vector)

            # loss = -torch.sum(torch.stack([log_softmax_scores[arcs[j]][j] for j in range(len(arcs))]))
            loss = -torch.sum(torch.stack([log_softmax_scores[arcs[j]][j] for j in range(len(arcs))]))
            # loss = -torch.sum(torch.stack([scores[arcs[j]][j] for j in range(len(arcs))])) / len(arcs)
            # loss = loss_function(scores, arcs)

            L += loss

            # Backpropagation
            loss.backward()

            if _i % GRAD_STEPS == 0: 
                # loss = loss / GRAD_STEPS
                optimizer.step()
                model.zero_grad()

            mst, _ = decode_mst(log_softmax_scores.detach().numpy(), log_softmax_scores.shape[0], has_labels=False)

            edges_count += log_softmax_scores.shape[0]
            correct_edges_count += sum(np.equal(mst[1:], arcs[1:]))

        accuracy = correct_edges_count / edges_count

        print("Epoch = ", epoch + 1, "/", EPOCHS)
        print("Loss = ", float(L))
        print("Accuracy = ", accuracy)

    # Save model to file
    saved_model_file_name = datetime.now().strftime("%y-%m-%d_%H-%M") + ".model"
    torch.save(model.state_dict(), saved_model_file_name)