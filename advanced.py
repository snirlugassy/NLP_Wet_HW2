# -*- coding: utf-8 -*-
"""NLP_Wet2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1b09TbwVyMQeD0v3luXddIscnZc-43st4
"""

# !cp "/content/drive/MyDrive/Technion/Semester 6/NLP/Homework/Wet 2/chu_liu_edmonds.py" .
# !cp "/content/drive/MyDrive/Technion/Semester 6/NLP/Homework/Wet 2/train.labeled" .
# !cp "/content/drive/MyDrive/Technion/Semester 6/NLP/Homework/Wet 2/test.labeled" .

import math
from itertools import permutations
from collections import defaultdict, OrderedDict
from datetime import datetime
from time import time

import numpy as np
import torchtext
import torch
from torch import nn, cuda, tensor, zeros, cat
from torch.optim import Adam, Adagrad
from torch.utils.data import DataLoader, Dataset
import torch.nn.functional as F

import matplotlib.pyplot as plt

from chu_liu_edmonds import decode_mst

from parameters import TRIM_TRAIN_DATASET
from parameters import HIDDEN_DIM
from parameters import WORD_EMBEDDING_DIM
from parameters import POS_EMBEDDING_DIM
from parameters import EPOCHS
from parameters import BATCH_SIZE
from parameters import LEARNING_RATE
from parameters import TEST_SAMPLE_SIZE
from parameters import ROOT
from parameters import OOV


train_path = "train.labeled"
test_path = "test.labeled"
model_file_name = "trained.model"
uas_file_name = "uas.npy"
loss_file_name = "loss.npy"


def get_vocab(sentences):
    word_dict = defaultdict(int)
    pos_dict = defaultdict(int)
    for sentence in sentences:
        for i in range(len(sentence)):
            word, pos = sentence[i][0], sentence[i][1]
            word_dict[word] += 1
            pos_dict[pos] += 1
    return word_dict, pos_dict


class ParsingDataset(Dataset):
    def __init__(self, train_sentences, all_sentences):
        self.sentences = train_sentences

        self.word_count, self.pos_count = get_vocab(all_sentences)

        # Set word id from vocab
        self.word_idx = defaultdict(lambda:0)
        # self.idx_word = dict()
        self.words_list = [OOV] + list(self.word_count.keys())
        for i in range(len(self.words_list)):
            w = self.words_list[i]
            self.word_idx[w] = i
            # self.idx_word[i] = w

        # Set POS id from vocab
        self.pos_idx = defaultdict(lambda:0)
        # self.idx_pos = dict()
        self.pos_list = list(self.pos_count.keys())
        for i in range(len(self.pos_list)):
            pos = self.pos_list[i]
            self.pos_idx[pos] = i
            # self.idx_pos[i] = pos

        self.word_vocab_size, self.pos_vocab_size = len(self.words_list), len(self.pos_list)

    def vectorize_tokens(self, tokens):
        idxs = [self.word_idx[w] for w in tokens]
        return tensor([idxs])

    def vectorize_pos(self, pos):
        pos_vector = torch.tensor([self.pos_idx[p] for p in pos])
        # pos_vector = zeros((len(pos), len(self.pos_idx.keys())), dtype=torch.int32)
        # for i in range(len(pos)):
        # pos_vector[(i, self.pos_idx[pos[i]])] = 1
        return pos_vector

    def __len__(self):
        return len(self.sentences)

    def __getitem__(self, idx):
        s = self.sentences[idx]
        tokens_vector = self.vectorize_tokens([w[0] for w in s])
        pos_vector = self.vectorize_pos([w[1] for w in s])
        # arcs = [(int(s[i][2]), i) for i in range(len(s))]
        arcs = [int(s[i][2]) for i in range(len(s))]
        return (tokens_vector, pos_vector), tensor(arcs)


def extract_sentences(file_path):
    sentences = []
    with open(file_path, 'r') as f:
        cur_sentence = [(ROOT, ROOT, -1)]
        for line in f:
            if line != '\n':
                splitted = line.split('\t')
                cur_sentence.append((splitted[1], splitted[3], splitted[6]))
            else:
                sentences.append(cur_sentence)
                cur_sentence = [(ROOT, ROOT, -1)]
    return sentences

train_sentences = extract_sentences(train_path)
test_sentences = extract_sentences(test_path)

sentences = train_sentences + test_sentences

if TRIM_TRAIN_DATASET > 0:
    train_dataset = ParsingDataset(train_sentences[:TRIM_TRAIN_DATASET], sentences)
else:
    train_dataset = ParsingDataset(train_sentences, sentences)

test_dataset = ParsingDataset(test_sentences, sentences)

# import matplotlib.pyplot as plt
#
# arc_lengths = list()
# for i in range(len(train_dataset)):
#     _, arcs = train_dataset[i]
#     for j in range(len(arcs)):
#         arc_lengths.append(int(abs(j-arcs[j])))
#
#
# plt.figure(figsize=(10,10))
# plt.title("Distance between head and modifier histogram")
# plt.xlabel("Distance")
# plt.ylabel("Number of pairs")
# plt.hist(arc_lengths, bins=max(arc_lengths))
# plt.xlim(0, 10)
# plt.show()
#
# import matplotlib.pyplot as plt
#
# arcs2d = list()
# for i in range(len(train_dataset)):
#     _, arcs = train_dataset[i]
#     for j in range(len(arcs)):
#         arcs2d.append((arcs[j], j))
#
#
# plt.figure(figsize=(10,10))
# plt.title("Head-modifier pairs scatter plot")
# plt.xlabel("Head index")
# plt.ylabel("Modifier index")
# plt.scatter(*zip(*arcs2d), )
# plt.show()


class DependencyParsingNetwork(nn.Module):
    def __init__(self, word_vocab_size, pos_vocab_size):
        super(DependencyParsingNetwork, self).__init__()

        # EMBEDDING
        self.word_embedding = nn.Embedding(word_vocab_size, WORD_EMBEDDING_DIM, padding_idx=0)
        self.pos_embedding = nn.Embedding(pos_vocab_size, POS_EMBEDDING_DIM, padding_idx=0)

        # RNN
        self.rnn = nn.GRU(input_size=WORD_EMBEDDING_DIM + POS_EMBEDDING_DIM, hidden_size=HIDDEN_DIM,
                            num_layers=2,
                            bidirectional=True, batch_first=True, dropout=0.02)

        # FC
        self.post_seq = nn.Sequential(
            nn.Linear(2 * 2 * HIDDEN_DIM, 2*HIDDEN_DIM),
            nn.Tanh(),
            nn.Linear(2*HIDDEN_DIM, HIDDEN_DIM),
            nn.Tanh(),
            nn.Linear(HIDDEN_DIM, 1)
        )

        self.softmax = nn.Softmax(dim=0)

    def forward(self, token_vector, pos_vector):
        _tokens = self.word_embedding(token_vector).squeeze(0)
        _pos = self.pos_embedding(pos_vector).squeeze(0)
        x = cat((_tokens, _pos), dim=1)
        x = x.unsqueeze(0)
        x, hn = self.rnn(x)
        x = x.squeeze(0)
        scores = zeros(x.shape[0], x.shape[0])
        for t1, t2 in permutations(range(len(x)), 2):
            scores[t1][t2] = self.post_seq(cat((x[t1], x[t2])))
        scores = self.softmax(scores)
        return scores

def test_accuracy(model, test_data, sample_size=10):
    with torch.no_grad():
        edges_count = 0
        correct_edges_count = 0
        random_test_idx = torch.randint(len(test_data), (sample_size,))
        for i in random_test_idx:
            (tokens_vector, pos_vector), arcs = test_data[i]
            tokens_vector = tokens_vector.to(device)
            pos_vector = pos_vector.to(device)
            arc = arcs.to(device)
            scores = model(tokens_vector, pos_vector)
            mst, _ = decode_mst(scores.detach().numpy(), scores.shape[0], has_labels=False)
            edges_count += scores.shape[0]
            correct_edges_count += sum(np.equal(mst, arcs))
        accuracy = correct_edges_count / edges_count
        return accuracy


if __name__ == "__main__":
    device = 'cuda' if cuda.is_available() else 'cpu'
    print("Device = ", device)

    model = DependencyParsingNetwork(train_dataset.word_vocab_size, train_dataset.pos_vocab_size)
    model = model.to(device)

    optimizer = Adam(model.parameters(), lr=LEARNING_RATE)

    avg_uas_history = list()
    loss_history = list()

    for epoch in range(EPOCHS):
        t0 = time()
        L = 0
        edges_count = 0
        correct_edges_count = 0
        model.zero_grad()
        uas = list()
        random_batch_idx = torch.randint(len(train_dataset), (BATCH_SIZE,))
        for i in random_batch_idx:
            (tokens_vector, pos_vector), arcs = train_dataset[i]
            tokens_vector = tokens_vector.to(device)
            pos_vector = pos_vector.to(device)

            # Forward
            scores = model(tokens_vector, pos_vector)

            log_softmax_scores = F.log_softmax(scores, dim=0)
            loss = -torch.sum(torch.stack([log_softmax_scores[arcs[j]][j] for j in range(len(arcs))]))
            # loss = loss_function(log_softmax_scores[1:, :], arcs[1:])
            loss.backward()

            # # loss = loss_function(softmax_scores, )
            # loss.backward()

            L += loss

            mst, _ = decode_mst(scores.detach().numpy(), scores.shape[0], has_labels=False)
            #
            # edges_count += scores.shape[0]
            uas.append(sum(np.equal(mst[1:], arcs[1:])) / (len(arcs) - 1))

        for param in model.parameters():
            param = param / BATCH_SIZE

        optimizer.step()

        test_acc = test_accuracy(model, test_dataset, TEST_SAMPLE_SIZE)
        uas = np.mean(uas)

        avg_uas_history.append(float(uas))
        loss_history.append(float(L))

        print("-------------------------")
        print("> Epoch = ", epoch + 1, "/", EPOCHS, "took", time() - t0, "seconds")
        print("> Loss = ", float(L))
        print("> Train Accuracy = ", float(uas))
        print("> Test Accuracy = ", float(test_acc) )

    # Save results
    plt.gcf()
    _x = range(1,len(loss_history)+1)
    plt.plot(_x, loss_history)
    plt.xticks(_x)
    plt.title("Loss per epoch")
    plt.xlabel("epoch")
    plt.ylabel("loss")
    plt.savefig("loss.png")

    plt.gcf()
    _x = range(1, len(avg_uas_history) + 1)
    plt.plot(_x, avg_uas_history)
    plt.xticks(_x)
    plt.title("UAS average per epoch")
    plt.xlabel("epoch")
    plt.ylabel("UAS")
    plt.savefig("uas.png")

    np.save(uas_file_name, avg_uas_history)
    np.save(loss_file_name, loss_history)
    torch.save(model.state_dict(), model_file_name)