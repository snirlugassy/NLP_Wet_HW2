# -*- coding: utf-8 -*-
"""NLP_Wet2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1b09TbwVyMQeD0v3luXddIscnZc-43st4
"""

# from google.colab import drive
import torch
from torch import nn, cuda, tensor, zeros, cat
from torch.optim import Adam
from torch.utils.data import DataLoader, Dataset
import torch.nn.functional as F
import numpy as np
import torchtext
from collections import defaultdict, OrderedDict
from itertools import combinations
from chu_liu_edmonds import decode_mst
from datetime import datetime

from send_email import send_email

train_path = "train.labeled"
test_path = "test.labeled"

ROOT = "_R_"


def get_vocab(sentences):
    word_dict = defaultdict(int)
    pos_dict = defaultdict(int)
    for sentence in sentences:
        for i in range(len(sentence)):
            word, pos = sentence[i][0], sentence[i][1]
            word_dict[word] += 1
            pos_dict[pos] += 1
    return word_dict, pos_dict


class ParsingDataset(Dataset):
    def __init__(self, sentences):
        self.sentences = sentences

        self.word_count, self.pos_count = get_vocab(train_nohead + test_nohead)

        # Set word id from vocab
        self.word_idx = dict()
        self.idx_word = dict()
        self.words_list = list(self.word_count.keys())
        for i in range(len(self.words_list)):
            w = self.words_list[i]
            self.word_idx[w] = i
            self.idx_word[i] = w

        # Set POS id from vocab
        self.pos_idx = dict()
        self.idx_pos = dict()
        self.pos_list = list(self.pos_count.keys())
        for i in range(len(self.pos_list)):
            pos = self.pos_list[i]
            self.pos_idx[pos] = i
            self.idx_pos[i] = pos

        self.word_vocab_size, self.pos_vocab_size = len(self.words_list), len(self.pos_list)

    def vectorize_tokens(self, tokens):
        idxs = [self.word_idx[w] for w in tokens]
        return tensor([idxs])

    def vectorize_pos(self, pos):
        pos_vector = zeros((len(pos), len(self.pos_idx.keys())))
        for i in range(len(pos)):
            pos_vector[(i, self.pos_idx[pos[i]])] = 1
        return pos_vector

    def __len__(self):
        return len(self.sentences)

    def __getitem__(self, idx):
        s = self.sentences[idx]
        tokens_vector = self.vectorize_tokens([w[0] for w in s])
        pos_vector = self.vectorize_pos([w[1] for w in s])
        # arcs = [(int(s[i][2]), i) for i in range(len(s))]
        arcs = [int(s[i][2]) for i in range(len(s))]
        return (tokens_vector, pos_vector), tensor(arcs)

def extract_sentences(file_path):
    sentences = []
    with open(file_path, 'r') as f:
        cur_sentence = [(ROOT, ROOT, -1)]
        for line in f:
            if line != '\n':
                splitted = line.split('\t')
                cur_sentence.append((splitted[1], splitted[3], splitted[6]))
            else:
                sentences.append(cur_sentence)
                cur_sentence = [(ROOT, ROOT, -1)]
    return sentences



train_sentences = extract_sentences(train_path)
test_sentences = extract_sentences(test_path)

train_nohead = []
for s in train_sentences:
    _s = [(x[0], x[1]) for x in s]
    train_nohead.append(_s)

test_nohead = []
for s in test_sentences:
    _s = [(x[0], x[1]) for x in s]
    test_nohead.append(_s)

# word_count, pos_count = get_vocab(train_nohead + test_nohead)
# word_vocab_size, pos_vocab_size = len(word_count.keys()), len(pos_count.keys())
#
# # Set word id from vocab
# word_idx = dict()
# idx_word = dict()
# words_list = list(word_count.keys())
# for i in range(len(words_list)):
#     w = words_list[i]
#     word_idx[w] = i
#     idx_word[i] = w
#
# # Set POS id from vocab
# pos_idx = dict()
# idx_pos = dict()
# pos_list = list(pos_count.keys())
# for i in range(len(pos_list)):
#     pos = pos_list[i]
#     pos_idx[pos] = i
#     idx_pos[i] = pos


class DependencyParsingNetwork(nn.Module):
    def __init__(self, hidden_dim, word_vocab_size, word_embedding_dim, pos_vocab_size):
        super(DependencyParsingNetwork, self).__init__()
        self.word_embedding = nn.Embedding(word_vocab_size, word_embedding_dim, padding_idx=0, max_norm=1)
        # TODO: add dropout arg to lstm
        self.lstm = nn.LSTM(input_size=word_embedding_dim + pos_vocab_size, hidden_size=hidden_dim, num_layers=2, bidirectional=True, batch_first=True)
        self.mlp = nn.Sequential(
            nn.Linear(2 * 2 * HIDDEN_DIM, 1),
            nn.Tanh(),
            # nn.Linear(100, 1),
        )
        self.log_softmax = nn.LogSoftmax(dim=0)

    def forward(self, token_vector, pos_vector):
        x = cat((self.word_embedding(token_vector).squeeze(0), pos_vector) , dim=1)
        x = x.unsqueeze(0)
        x, (hn, cn) = self.lstm(x)
        x = x.squeeze(0)
        scores = zeros(x.shape[0], x.shape[0])
        for _i, _j in combinations(range(len(x)), 2):
            scores[_i][_j] = self.mlp(cat((x[_i],x[_j])))
        _scores = self.log_softmax(scores)
        return scores, _scores


def vectorize_tokens(tokens, to_idx):
    idxs = [to_idx[w] for w in tokens]
    return tensor([idxs])


def vectorize_pos(pos, to_idx):
    pos_vector = zeros((len(pos), len(to_idx.keys())))
    for i in range(len(pos)):
        pos_vector[(i, to_idx[pos[i]])] = 1
    return pos_vector


HIDDEN_DIM = 250
WORD_EMBEDDING_DIM = 1000
EPOCHS = 1000
GRAD_STEPS = 5
TRIM_TRAIN_DATASET = 20
BATCH_SIZE = 10
LEARNING_RATE = 0.01

if TRIM_TRAIN_DATASET > 0:
    train_dataset = ParsingDataset(train_sentences[:TRIM_TRAIN_DATASET])
else:
    train_dataset = ParsingDataset(train_sentences)

data_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)

device = 'cuda' if cuda.is_available() else 'cpu'
print("Device = ", device)

model = DependencyParsingNetwork(HIDDEN_DIM, train_dataset.word_vocab_size, WORD_EMBEDDING_DIM,  train_dataset.pos_vocab_size)
model = model.to(device)

# loss = nn.NLLLoss()
optimizer = Adam(model.parameters(), lr=LEARNING_RATE)
loss_function = nn.NLLLoss()
log_softmax = nn.LogSoftmax(dim=1)


if __name__ == "__main__":
    for epoch in range(EPOCHS):
        L = 0
        edges_count = 0
        correct_edges_count = 0
        random_batch_idx = torch.randint(len(train_dataset), (BATCH_SIZE,))
        _i = -1
        for i in random_batch_idx:
            _i += 1
            (tokens_vector, pos_vector), arcs = train_dataset[i]
            tokens_vector = tokens_vector.to(device)
            pos_vector = pos_vector.to(device)
            arc = arcs.to(device)

            # Forward
            scores, log_softmax_scores = model(tokens_vector, pos_vector)

            loss = -torch.sum(torch.stack([log_softmax_scores[arcs[j]][j] for j in range(len(arcs))])) / len(arcs)
            # loss = loss_function(scores[1:,], arcs[1:])

            L += loss

            # Backpropagation
            loss.backward()

            if _i % GRAD_STEPS == 0:
                optimizer.step()
                model.zero_grad()

            mst, _ = decode_mst(scores.detach().numpy(), scores.shape[0], has_labels=False)

            edges_count += scores.shape[0]
            correct_edges_count += sum(np.equal(mst, arcs))

        accuracy = correct_edges_count / edges_count

        print("Epoch = ", epoch, "/", EPOCHS)
        print("Loss = ", L)
        print("Accuracy = ", accuracy)

    # saved_model_file_name = datetime.now().strftime("%y-%m-%d_%H-%M") + ".model"
    # torch.save(model, saved_model_file_name)
    #
    # # Email notification
    # msg = """
    # Current loss = {} <br/>
    # Accuracy = ??? <br/>
    # Model parameters saved to {} <br/>
    # """.format(str(L), saved_model_file_name)
    # send_email("NLP HW2 Run finished", msg)