# -*- coding: utf-8 -*-
"""NLP_Wet2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1b09TbwVyMQeD0v3luXddIscnZc-43st4
"""

# !cp "/content/drive/MyDrive/Technion/Semester 6/NLP/Homework/Wet 2/chu_liu_edmonds.py" .
# !cp "/content/drive/MyDrive/Technion/Semester 6/NLP/Homework/Wet 2/train.labeled" .
# !cp "/content/drive/MyDrive/Technion/Semester 6/NLP/Homework/Wet 2/test.labeled" .

import math
from itertools import permutations
from collections import defaultdict, OrderedDict
from datetime import datetime
from time import time

import numpy as np
import torchtext
import torch
from torch import nn, cuda, tensor, zeros, cat
from torch.optim import Adam, Adagrad
from torch.utils.data import DataLoader, Dataset
import torch.nn.functional as F

from chu_liu_edmonds import decode_mst

train_path = "train.labeled"
test_path = "test.labeled"
ROOT = "_R_"

def get_vocab(sentences):
    word_dict = defaultdict(int)
    pos_dict = defaultdict(int)
    for sentence in sentences:
        for i in range(len(sentence)):
            word, pos = sentence[i][0], sentence[i][1]
            word_dict[word] += 1
            pos_dict[pos] += 1
    return word_dict, pos_dict


class ParsingDataset(Dataset):
    def __init__(self, train_sentences, all_sentences):
        self.sentences = train_sentences

        self.word_count, self.pos_count = get_vocab(all_sentences)

        # Set word id from vocab
        self.word_idx = dict()
        self.idx_word = dict()
        self.words_list = list(self.word_count.keys())
        for i in range(len(self.words_list)):
            w = self.words_list[i]
            self.word_idx[w] = i
            self.idx_word[i] = w

        # Set POS id from vocab
        self.pos_idx = dict()
        self.idx_pos = dict()
        self.pos_list = list(self.pos_count.keys())
        for i in range(len(self.pos_list)):
            pos = self.pos_list[i]
            self.pos_idx[pos] = i
            self.idx_pos[i] = pos

        self.word_vocab_size, self.pos_vocab_size = len(self.words_list), len(self.pos_list)

    def vectorize_tokens(self, tokens):
        idxs = [self.word_idx[w] for w in tokens]
        return tensor([idxs])

    def vectorize_pos(self, pos):
        pos_vector = torch.tensor([self.pos_idx[p] for p in pos])
        # pos_vector = zeros((len(pos), len(self.pos_idx.keys())), dtype=torch.int32)
        # for i in range(len(pos)):
        # pos_vector[(i, self.pos_idx[pos[i]])] = 1
        return pos_vector

    def __len__(self):
        return len(self.sentences)

    def __getitem__(self, idx):
        s = self.sentences[idx]
        tokens_vector = self.vectorize_tokens([w[0] for w in s])
        pos_vector = self.vectorize_pos([w[1] for w in s])
        # arcs = [(int(s[i][2]), i) for i in range(len(s))]
        arcs = [int(s[i][2]) for i in range(len(s))]
        return (tokens_vector, pos_vector), tensor(arcs)


def extract_sentences(file_path):
    sentences = []
    with open(file_path, 'r') as f:
        cur_sentence = [(ROOT, ROOT, -1)]
        for line in f:
            if line != '\n':
                splitted = line.split('\t')
                cur_sentence.append((splitted[1], splitted[3], splitted[6]))
            else:
                sentences.append(cur_sentence)
                cur_sentence = [(ROOT, ROOT, -1)]
    return sentences

train_sentences = extract_sentences(train_path)
test_sentences = extract_sentences(test_path)

sentences = train_sentences + test_sentences

TRIM_TRAIN_DATASET = 1000

if TRIM_TRAIN_DATASET > 0:
    train_dataset = ParsingDataset(train_sentences[:TRIM_TRAIN_DATASET], sentences)
else:
    train_dataset = ParsingDataset(train_sentences, sentences)

test_dataset = ParsingDataset(test_sentences, sentences)

import matplotlib.pyplot as plt

arc_lengths = list()
for i in range(len(train_dataset)):
    _, arcs = train_dataset[i]
    for j in range(len(arcs)):
        arc_lengths.append(int(abs(j-arcs[j])))


plt.figure(figsize=(10,10))
plt.title("Distance between head and modifier histogram")
plt.xlabel("Distance")
plt.ylabel("Number of pairs")
plt.hist(arc_lengths, bins=max(arc_lengths))
plt.xlim(0, 10)
plt.show()

import matplotlib.pyplot as plt

arcs2d = list()
for i in range(len(train_dataset)):
    _, arcs = train_dataset[i]
    for j in range(len(arcs)):
        arcs2d.append((arcs[j], j))


plt.figure(figsize=(10,10))
plt.title("Head-modifier pairs scatter plot")
plt.xlabel("Head index")
plt.ylabel("Modifier index")
plt.scatter(*zip(*arcs2d), )
plt.show()

LSTM_HIDDEN_DIM = 125
WORD_EMBEDDING_DIM = 200
POS_EMBEDDING_DIM = 15

class DependencyParsingNetwork(nn.Module):
    def __init__(self, word_vocab_size, pos_vocab_size):
        super(DependencyParsingNetwork, self).__init__()
        
        # EMBEDDING
        self.word_embedding = nn.Embedding(word_vocab_size, WORD_EMBEDDING_DIM, padding_idx=0)
        self.pos_embedding = nn.Embedding(pos_vocab_size, POS_EMBEDDING_DIM)

        # LSTM
        self.lstm = nn.LSTM(input_size=WORD_EMBEDDING_DIM + POS_EMBEDDING_DIM, hidden_size=LSTM_HIDDEN_DIM, num_layers=4,
                            bidirectional=True, batch_first=True, dropout=0.05)
        
        # FC
        self.post_seq = nn.Sequential(
            nn.Linear(2 * 2 * LSTM_HIDDEN_DIM, LSTM_HIDDEN_DIM),
            nn.Tanh(),
            nn.Linear(LSTM_HIDDEN_DIM, 100),
            nn.Tanh(),
            nn.Linear(100, 100),
            nn.Tanh(),
            nn.Linear(100, 100),
            nn.Tanh(),
            nn.Linear(100, 1),
        )

        self.log_softmax = nn.LogSoftmax(dim=0)

    def forward(self, token_vector, pos_vector):
        _tokens = self.word_embedding(token_vector).squeeze(0)
        _pos = self.pos_embedding(pos_vector).squeeze(0)
        x = cat((_tokens, _pos), dim=1)
        x = x.unsqueeze(0)
        x, (hn, cn) = self.lstm(x)
        x = x.squeeze(0)
        scores = zeros(x.shape[0], x.shape[0])
        for t1, t2 in permutations(range(len(x)), 2):
            scores[t1][t2] = self.post_seq(cat((x[t1], x[t2])))
        _scores = self.log_softmax(scores)
        return scores, _scores

def test_accuracy(model, test_data, sample_size=50):
    edges_count = 0
    correct_edges_count = 0
    random_test_idx = torch.randint(len(test_data), (sample_size,))
    for i in random_test_idx:
        (tokens_vector, pos_vector), arcs = test_data[i]
        tokens_vector = tokens_vector.to(device)
        pos_vector = pos_vector.to(device)
        arc = arcs.to(device)
        scores, log_softmax_scores = model(tokens_vector, pos_vector)
        mst, _ = decode_mst(scores.detach().numpy(), scores.shape[0], has_labels=False)
        edges_count += scores.shape[0]
        correct_edges_count += sum(np.equal(mst, arcs))
    accuracy = correct_edges_count / edges_count
    return accuracy


device = 'cuda' if cuda.is_available() else 'cpu'
print("Device = ", device)

EPOCHS = 100
BATCH_SIZE = 50
LEARNING_RATE = 0.001
TEST_SAMPLE_SIZE = 5

if __name__ == "__main__":
    model_file_name = "trained.model"
    model = DependencyParsingNetwork(train_dataset.word_vocab_size, train_dataset.pos_vocab_size)
    # try:
    #     model.load_state_dict(torch.load(model_file_name))
    # except FileNotFoundError:
    #     print("No saved model state, starting a new model")

    model = model.to(device)

    optimizer = Adam(model.parameters(), lr=LEARNING_RATE)

    for epoch in range(EPOCHS):
        t0 = time()
        L = 0
        edges_count = 0
        correct_edges_count = 0
        model.zero_grad()

        random_batch_idx = torch.randint(len(train_dataset), (BATCH_SIZE,))
        for i in random_batch_idx:

            (tokens_vector, pos_vector), arcs = train_dataset[i]
            tokens_vector = tokens_vector.to(device)
            pos_vector = pos_vector.to(device)

            # Forward
            scores, log_softmax_scores = model(tokens_vector, pos_vector)

            loss = -torch.sum(torch.stack([log_softmax_scores[arcs[j]][j] for j in range(len(arcs))])) / len(arcs)

            L += loss

            # Backpropagation
            loss.backward()

            mst, _ = decode_mst(log_softmax_scores.detach().numpy(), log_softmax_scores.shape[0], has_labels=False)

            edges_count += log_softmax_scores.shape[0]
            correct_edges_count += sum(np.equal(mst[1:], arcs[1:]))

        loss = loss / BATCH_SIZE
        optimizer.step()

        # test_acc = test_accuracy(model, test_dataset, TEST_SAMPLE_SIZE)
        train_acc = correct_edges_count / edges_count
        
        print("-------------------------")
        print("> Epoch = ", epoch + 1, "/", EPOCHS, "took", time() - t0, "seconds")
        print("> Loss = ", float(L))
        print("> Train Accuracy = ", float(train_acc))
        # print("> Test Accuracy = ", float(test_acc) )

    # Save model to file
    # saved_model_file_name = datetime.now().strftime("%y-%m-%d_%H-%M") + ".model"
    torch.save(model.state_dict(), model_file_name)

# !cp trained.model drive/MyDrive/